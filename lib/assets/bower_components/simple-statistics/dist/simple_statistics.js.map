{
  "version": 3,
  "sources": [
    "node_modules/browserify/node_modules/browser-pack/_prelude.js",
    "index.js",
    "src/bayesian_classifier.js",
    "src/bernoulli_distribution.js",
    "src/binomial_distribution.js",
    "src/chi_squared_distribution_table.js",
    "src/chi_squared_goodness_of_fit.js",
    "src/chunk.js",
    "src/ckmeans.js",
    "src/cumulative_std_normal_probability.js",
    "src/epsilon.js",
    "src/error_function.js",
    "src/factorial.js",
    "src/geometric_mean.js",
    "src/harmonic_mean.js",
    "src/interquartile_range.js",
    "src/inverse_error_function.js",
    "src/linear_regression.js",
    "src/linear_regression_line.js",
    "src/mad.js",
    "src/max.js",
    "src/mean.js",
    "src/median.js",
    "src/min.js",
    "src/mixin.js",
    "src/mode.js",
    "src/numeric_sort.js",
    "src/perceptron.js",
    "src/poisson_distribution.js",
    "src/probit.js",
    "src/quantile.js",
    "src/quantile_sorted.js",
    "src/r_squared.js",
    "src/root_mean_square.js",
    "src/sample.js",
    "src/sample_correlation.js",
    "src/sample_covariance.js",
    "src/sample_skewness.js",
    "src/sample_standard_deviation.js",
    "src/sample_variance.js",
    "src/shuffle.js",
    "src/shuffle_in_place.js",
    "src/sorted_unique_count.js",
    "src/standard_deviation.js",
    "src/standard_normal_table.js",
    "src/sum.js",
    "src/sum_nth_power_deviations.js",
    "src/t_test.js",
    "src/t_test_two_sample.js",
    "src/variance.js",
    "src/z_score.js"
  ],
  "names": [],
  "mappings": "AAAA;ACAA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;ACpEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;ACrHA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AC1BA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;ACnDA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;ACtDA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;ACzGA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AC3CA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;ACrLA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AC1CA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;ACvCA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;ACpCA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AChCA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;ACrDA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;ACpCA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AC1BA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AC1BA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;ACvEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AC3BA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AC/BA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;ACzBA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;ACzBA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AC1CA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;ACvBA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AC9EA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AClEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AC9BA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AC/FA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AC9CA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AC5BA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;ACpDA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;ACzCA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;ACnDA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AC3BA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AC7BA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AC9BA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AClDA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;ACjCA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;ACtBA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;ACpCA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AC1BA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;ACtDA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AC7BA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AC5BA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;ACpCA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;ACtBA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AC9BA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;ACtCA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;ACvDA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AC3BA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA",
  "file": "generated.js",
  "sourceRoot": "",
  "sourcesContent": [
    "(function e(t,n,r){function s(o,u){if(!n[o]){if(!t[o]){var a=typeof require==\"function\"&&require;if(!u&&a)return a(o,!0);if(i)return i(o,!0);var f=new Error(\"Cannot find module '\"+o+\"'\");throw f.code=\"MODULE_NOT_FOUND\",f}var l=n[o]={exports:{}};t[o][0].call(l.exports,function(e){var n=t[o][1][e];return s(n?n:e)},l,l.exports,e,t,n,r)}return n[o].exports}var i=typeof require==\"function\"&&require;for(var o=0;o<r.length;o++)s(r[o]);return s})",
    "'use strict';\n\n// # simple-statistics\n//\n// A simple, literate statistics system.\n\nvar ss = module.exports = {};\n\n// Linear Regression\nss.linearRegression = require(17);\nss.linearRegressionLine = require(18);\nss.standardDeviation = require(43);\nss.rSquared = require(32);\nss.mode = require(25);\nss.min = require(23);\nss.max = require(20);\nss.sum = require(45);\nss.quantile = require(30);\nss.quantileSorted = require(31);\nss.iqr = ss.interquartileRange = require(15);\nss.medianAbsoluteDeviation = ss.mad = require(19);\nss.chunk = require(7);\nss.shuffle = require(40);\nss.shuffleInPlace = require(41);\nss.sample = require(34);\nss.ckmeans = require(8);\nss.sortedUniqueCount = require(42);\nss.sumNthPowerDeviations = require(46);\n\n// sample statistics\nss.sampleCovariance = require(36);\nss.sampleCorrelation = require(35);\nss.sampleVariance = require(39);\nss.sampleStandardDeviation = require(38);\nss.sampleSkewness = require(37);\n\n// measures of centrality\nss.geometricMean = require(13);\nss.harmonicMean = require(14);\nss.mean = ss.average = require(21);\nss.median = require(22);\n\nss.rootMeanSquare = ss.rms = require(33);\nss.variance = require(49);\nss.tTest = require(47);\nss.tTestTwoSample = require(48);\n// ss.jenks = require('./src/jenks');\n\n// Classifiers\nss.bayesian = require(2);\nss.perceptron = require(27);\n\n// Distribution-related methods\nss.epsilon = require(10); // We make ε available to the test suite.\nss.factorial = require(12);\nss.bernoulliDistribution = require(3);\nss.binomialDistribution = require(4);\nss.poissonDistribution = require(28);\nss.chiSquaredGoodnessOfFit = require(6);\n\n// Normal distribution\nss.zScore = require(50);\nss.cumulativeStdNormalProbability = require(9);\nss.standardNormalTable = require(44);\nss.errorFunction = ss.erf = require(11);\nss.inverseErrorFunction = require(16);\nss.probit = require(29);\nss.mixin = require(24);\n",
    "'use strict';\n\n/**\n * [Bayesian Classifier](http://en.wikipedia.org/wiki/Naive_Bayes_classifier)\n *\n * This is a naïve bayesian classifier that takes\n * singly-nested objects.\n *\n * @class\n * @example\n * var bayes = new BayesianClassifier();\n * bayes.train({\n *   species: 'Cat'\n * }, 'animal');\n * var result = bayes.score({\n *   species: 'Cat'\n * })\n * // result\n * // {\n * //   animal: 1\n * // }\n */\nfunction BayesianClassifier() {\n    // The number of items that are currently\n    // classified in the model\n    this.totalCount = 0;\n    // Every item classified in the model\n    this.data = {};\n}\n\n/**\n * Train the classifier with a new item, which has a single\n * dimension of Javascript literal keys and values.\n *\n * @param {Object} item an object with singly-deep properties\n * @param {string} category the category this item belongs to\n * @return {undefined} adds the item to the classifier\n */\nBayesianClassifier.prototype.train = function(item, category) {\n    // If the data object doesn't have any values\n    // for this category, create a new object for it.\n    if (!this.data[category]) {\n        this.data[category] = {};\n    }\n\n    // Iterate through each key in the item.\n    for (var k in item) {\n        var v = item[k];\n        // Initialize the nested object `data[category][k][item[k]]`\n        // with an object of keys that equal 0.\n        if (this.data[category][k] === undefined) {\n            this.data[category][k] = {};\n        }\n        if (this.data[category][k][v] === undefined) {\n            this.data[category][k][v] = 0;\n        }\n\n        // And increment the key for this key/value combination.\n        this.data[category][k][item[k]]++;\n    }\n\n    // Increment the number of items classified\n    this.totalCount++;\n};\n\n/**\n * Generate a score of how well this item matches all\n * possible categories based on its attributes\n *\n * @param {Object} item an item in the same format as with train\n * @returns {Object} of probabilities that this item belongs to a\n * given category.\n */\nBayesianClassifier.prototype.score = function(item) {\n    // Initialize an empty array of odds per category.\n    var odds = {}, category;\n    // Iterate through each key in the item,\n    // then iterate through each category that has been used\n    // in previous calls to `.train()`\n    for (var k in item) {\n        var v = item[k];\n        for (category in this.data) {\n            // Create an empty object for storing key - value combinations\n            // for this category.\n            if (odds[category] === undefined) { odds[category] = {}; }\n\n            // If this item doesn't even have a property, it counts for nothing,\n            // but if it does have the property that we're looking for from\n            // the item to categorize, it counts based on how popular it is\n            // versus the whole population.\n            if (this.data[category][k]) {\n                odds[category][k + '_' + v] = (this.data[category][k][v] || 0) / this.totalCount;\n            } else {\n                odds[category][k + '_' + v] = 0;\n            }\n        }\n    }\n\n    // Set up a new object that will contain sums of these odds by category\n    var oddsSums = {};\n\n    for (category in odds) {\n        // Tally all of the odds for each category-combination pair -\n        // the non-existence of a category does not add anything to the\n        // score.\n        for (var combination in odds[category]) {\n            if (oddsSums[category] === undefined) {\n                oddsSums[category] = 0;\n            }\n            oddsSums[category] += odds[category][combination];\n        }\n    }\n\n    return oddsSums;\n};\n\nmodule.exports = BayesianClassifier;\n",
    "'use strict';\n\nvar binomialDistribution = require(4);\n\n/**\n * The [Bernoulli distribution](http://en.wikipedia.org/wiki/Bernoulli_distribution)\n * is the probability discrete\n * distribution of a random variable which takes value 1 with success\n * probability `p` and value 0 with failure\n * probability `q` = 1 - `p`. It can be used, for example, to represent the\n * toss of a coin, where \"1\" is defined to mean \"heads\" and \"0\" is defined\n * to mean \"tails\" (or vice versa). It is\n * a special case of a Binomial Distribution\n * where `n` = 1.\n *\n * @param {number} p input value, between 0 and 1 inclusive\n * @returns {number} value of bernoulli distribution at this point\n */\nfunction bernoulliDistribution(p) {\n    // Check that `p` is a valid probability (0 ≤ p ≤ 1)\n    if (p < 0 || p > 1 ) { return null; }\n\n    return binomialDistribution(1, p);\n}\n\nmodule.exports = bernoulliDistribution;\n",
    "'use strict';\n\nvar epsilon = require(10);\nvar factorial = require(12);\n\n/**\n * The [Binomial Distribution](http://en.wikipedia.org/wiki/Binomial_distribution) is the discrete probability\n * distribution of the number of successes in a sequence of n independent yes/no experiments, each of which yields\n * success with probability `probability`. Such a success/failure experiment is also called a Bernoulli experiment or\n * Bernoulli trial; when trials = 1, the Binomial Distribution is a Bernoulli Distribution.\n *\n * @param {number} trials number of trials to simulate\n * @param {number} probability\n * @returns {number} output\n */\nfunction binomialDistribution(trials, probability) {\n    // Check that `p` is a valid probability (0 ≤ p ≤ 1),\n    // that `n` is an integer, strictly positive.\n    if (probability < 0 || probability > 1 ||\n        trials <= 0 || trials % 1 !== 0) {\n        return null;\n    }\n\n    // We initialize `x`, the random variable, and `accumulator`, an accumulator\n    // for the cumulative distribution function to 0. `distribution_functions`\n    // is the object we'll return with the `probability_of_x` and the\n    // `cumulativeProbability_of_x`, as well as the calculated mean &\n    // variance. We iterate until the `cumulativeProbability_of_x` is\n    // within `epsilon` of 1.0.\n    var x = 0,\n        cumulativeProbability = 0,\n        cells = {};\n\n    // This algorithm iterates through each potential outcome,\n    // until the `cumulativeProbability` is very close to 1, at\n    // which point we've defined the vast majority of outcomes\n    do {\n        // a [probability mass function](https://en.wikipedia.org/wiki/Probability_mass_function)\n        cells[x] = factorial(trials) /\n            (factorial(x) * factorial(trials - x)) *\n            (Math.pow(probability, x) * Math.pow(1 - probability, trials - x));\n        cumulativeProbability += cells[x];\n        x++;\n    // when the cumulativeProbability is nearly 1, we've calculated\n    // the useful range of this distribution\n    } while (cumulativeProbability < 1 - epsilon);\n\n    return cells;\n}\n\nmodule.exports = binomialDistribution;\n",
    "'use strict';\n\n/**\n * **Percentage Points of the χ2 (Chi-Squared) Distribution**\n *\n * The [χ2 (Chi-Squared) Distribution](http://en.wikipedia.org/wiki/Chi-squared_distribution) is used in the common\n * chi-squared tests for goodness of fit of an observed distribution to a theoretical one, the independence of two\n * criteria of classification of qualitative data, and in confidence interval estimation for a population standard\n * deviation of a normal distribution from a sample standard deviation.\n *\n * Values from Appendix 1, Table III of William W. Hines & Douglas C. Montgomery, \"Probability and Statistics in\n * Engineering and Management Science\", Wiley (1980).\n */\nvar chiSquaredDistributionTable = {\n    1: { 0.995:  0.00, 0.99:  0.00, 0.975:  0.00, 0.95:  0.00, 0.9:  0.02, 0.5:  0.45, 0.1:  2.71, 0.05:  3.84, 0.025:  5.02, 0.01:  6.63, 0.005:  7.88 },\n    2: { 0.995:  0.01, 0.99:  0.02, 0.975:  0.05, 0.95:  0.10, 0.9:  0.21, 0.5:  1.39, 0.1:  4.61, 0.05:  5.99, 0.025:  7.38, 0.01:  9.21, 0.005: 10.60 },\n    3: { 0.995:  0.07, 0.99:  0.11, 0.975:  0.22, 0.95:  0.35, 0.9:  0.58, 0.5:  2.37, 0.1:  6.25, 0.05:  7.81, 0.025:  9.35, 0.01: 11.34, 0.005: 12.84 },\n    4: { 0.995:  0.21, 0.99:  0.30, 0.975:  0.48, 0.95:  0.71, 0.9:  1.06, 0.5:  3.36, 0.1:  7.78, 0.05:  9.49, 0.025: 11.14, 0.01: 13.28, 0.005: 14.86 },\n    5: { 0.995:  0.41, 0.99:  0.55, 0.975:  0.83, 0.95:  1.15, 0.9:  1.61, 0.5:  4.35, 0.1:  9.24, 0.05: 11.07, 0.025: 12.83, 0.01: 15.09, 0.005: 16.75 },\n    6: { 0.995:  0.68, 0.99:  0.87, 0.975:  1.24, 0.95:  1.64, 0.9:  2.20, 0.5:  5.35, 0.1: 10.65, 0.05: 12.59, 0.025: 14.45, 0.01: 16.81, 0.005: 18.55 },\n    7: { 0.995:  0.99, 0.99:  1.25, 0.975:  1.69, 0.95:  2.17, 0.9:  2.83, 0.5:  6.35, 0.1: 12.02, 0.05: 14.07, 0.025: 16.01, 0.01: 18.48, 0.005: 20.28 },\n    8: { 0.995:  1.34, 0.99:  1.65, 0.975:  2.18, 0.95:  2.73, 0.9:  3.49, 0.5:  7.34, 0.1: 13.36, 0.05: 15.51, 0.025: 17.53, 0.01: 20.09, 0.005: 21.96 },\n    9: { 0.995:  1.73, 0.99:  2.09, 0.975:  2.70, 0.95:  3.33, 0.9:  4.17, 0.5:  8.34, 0.1: 14.68, 0.05: 16.92, 0.025: 19.02, 0.01: 21.67, 0.005: 23.59 },\n    10: { 0.995:  2.16, 0.99:  2.56, 0.975:  3.25, 0.95:  3.94, 0.9:  4.87, 0.5:  9.34, 0.1: 15.99, 0.05: 18.31, 0.025: 20.48, 0.01: 23.21, 0.005: 25.19 },\n    11: { 0.995:  2.60, 0.99:  3.05, 0.975:  3.82, 0.95:  4.57, 0.9:  5.58, 0.5: 10.34, 0.1: 17.28, 0.05: 19.68, 0.025: 21.92, 0.01: 24.72, 0.005: 26.76 },\n    12: { 0.995:  3.07, 0.99:  3.57, 0.975:  4.40, 0.95:  5.23, 0.9:  6.30, 0.5: 11.34, 0.1: 18.55, 0.05: 21.03, 0.025: 23.34, 0.01: 26.22, 0.005: 28.30 },\n    13: { 0.995:  3.57, 0.99:  4.11, 0.975:  5.01, 0.95:  5.89, 0.9:  7.04, 0.5: 12.34, 0.1: 19.81, 0.05: 22.36, 0.025: 24.74, 0.01: 27.69, 0.005: 29.82 },\n    14: { 0.995:  4.07, 0.99:  4.66, 0.975:  5.63, 0.95:  6.57, 0.9:  7.79, 0.5: 13.34, 0.1: 21.06, 0.05: 23.68, 0.025: 26.12, 0.01: 29.14, 0.005: 31.32 },\n    15: { 0.995:  4.60, 0.99:  5.23, 0.975:  6.27, 0.95:  7.26, 0.9:  8.55, 0.5: 14.34, 0.1: 22.31, 0.05: 25.00, 0.025: 27.49, 0.01: 30.58, 0.005: 32.80 },\n    16: { 0.995:  5.14, 0.99:  5.81, 0.975:  6.91, 0.95:  7.96, 0.9:  9.31, 0.5: 15.34, 0.1: 23.54, 0.05: 26.30, 0.025: 28.85, 0.01: 32.00, 0.005: 34.27 },\n    17: { 0.995:  5.70, 0.99:  6.41, 0.975:  7.56, 0.95:  8.67, 0.9: 10.09, 0.5: 16.34, 0.1: 24.77, 0.05: 27.59, 0.025: 30.19, 0.01: 33.41, 0.005: 35.72 },\n    18: { 0.995:  6.26, 0.99:  7.01, 0.975:  8.23, 0.95:  9.39, 0.9: 10.87, 0.5: 17.34, 0.1: 25.99, 0.05: 28.87, 0.025: 31.53, 0.01: 34.81, 0.005: 37.16 },\n    19: { 0.995:  6.84, 0.99:  7.63, 0.975:  8.91, 0.95: 10.12, 0.9: 11.65, 0.5: 18.34, 0.1: 27.20, 0.05: 30.14, 0.025: 32.85, 0.01: 36.19, 0.005: 38.58 },\n    20: { 0.995:  7.43, 0.99:  8.26, 0.975:  9.59, 0.95: 10.85, 0.9: 12.44, 0.5: 19.34, 0.1: 28.41, 0.05: 31.41, 0.025: 34.17, 0.01: 37.57, 0.005: 40.00 },\n    21: { 0.995:  8.03, 0.99:  8.90, 0.975: 10.28, 0.95: 11.59, 0.9: 13.24, 0.5: 20.34, 0.1: 29.62, 0.05: 32.67, 0.025: 35.48, 0.01: 38.93, 0.005: 41.40 },\n    22: { 0.995:  8.64, 0.99:  9.54, 0.975: 10.98, 0.95: 12.34, 0.9: 14.04, 0.5: 21.34, 0.1: 30.81, 0.05: 33.92, 0.025: 36.78, 0.01: 40.29, 0.005: 42.80 },\n    23: { 0.995:  9.26, 0.99: 10.20, 0.975: 11.69, 0.95: 13.09, 0.9: 14.85, 0.5: 22.34, 0.1: 32.01, 0.05: 35.17, 0.025: 38.08, 0.01: 41.64, 0.005: 44.18 },\n    24: { 0.995:  9.89, 0.99: 10.86, 0.975: 12.40, 0.95: 13.85, 0.9: 15.66, 0.5: 23.34, 0.1: 33.20, 0.05: 36.42, 0.025: 39.36, 0.01: 42.98, 0.005: 45.56 },\n    25: { 0.995: 10.52, 0.99: 11.52, 0.975: 13.12, 0.95: 14.61, 0.9: 16.47, 0.5: 24.34, 0.1: 34.28, 0.05: 37.65, 0.025: 40.65, 0.01: 44.31, 0.005: 46.93 },\n    26: { 0.995: 11.16, 0.99: 12.20, 0.975: 13.84, 0.95: 15.38, 0.9: 17.29, 0.5: 25.34, 0.1: 35.56, 0.05: 38.89, 0.025: 41.92, 0.01: 45.64, 0.005: 48.29 },\n    27: { 0.995: 11.81, 0.99: 12.88, 0.975: 14.57, 0.95: 16.15, 0.9: 18.11, 0.5: 26.34, 0.1: 36.74, 0.05: 40.11, 0.025: 43.19, 0.01: 46.96, 0.005: 49.65 },\n    28: { 0.995: 12.46, 0.99: 13.57, 0.975: 15.31, 0.95: 16.93, 0.9: 18.94, 0.5: 27.34, 0.1: 37.92, 0.05: 41.34, 0.025: 44.46, 0.01: 48.28, 0.005: 50.99 },\n    29: { 0.995: 13.12, 0.99: 14.26, 0.975: 16.05, 0.95: 17.71, 0.9: 19.77, 0.5: 28.34, 0.1: 39.09, 0.05: 42.56, 0.025: 45.72, 0.01: 49.59, 0.005: 52.34 },\n    30: { 0.995: 13.79, 0.99: 14.95, 0.975: 16.79, 0.95: 18.49, 0.9: 20.60, 0.5: 29.34, 0.1: 40.26, 0.05: 43.77, 0.025: 46.98, 0.01: 50.89, 0.005: 53.67 },\n    40: { 0.995: 20.71, 0.99: 22.16, 0.975: 24.43, 0.95: 26.51, 0.9: 29.05, 0.5: 39.34, 0.1: 51.81, 0.05: 55.76, 0.025: 59.34, 0.01: 63.69, 0.005: 66.77 },\n    50: { 0.995: 27.99, 0.99: 29.71, 0.975: 32.36, 0.95: 34.76, 0.9: 37.69, 0.5: 49.33, 0.1: 63.17, 0.05: 67.50, 0.025: 71.42, 0.01: 76.15, 0.005: 79.49 },\n    60: { 0.995: 35.53, 0.99: 37.48, 0.975: 40.48, 0.95: 43.19, 0.9: 46.46, 0.5: 59.33, 0.1: 74.40, 0.05: 79.08, 0.025: 83.30, 0.01: 88.38, 0.005: 91.95 },\n    70: { 0.995: 43.28, 0.99: 45.44, 0.975: 48.76, 0.95: 51.74, 0.9: 55.33, 0.5: 69.33, 0.1: 85.53, 0.05: 90.53, 0.025: 95.02, 0.01: 100.42, 0.005: 104.22 },\n    80: { 0.995: 51.17, 0.99: 53.54, 0.975: 57.15, 0.95: 60.39, 0.9: 64.28, 0.5: 79.33, 0.1: 96.58, 0.05: 101.88, 0.025: 106.63, 0.01: 112.33, 0.005: 116.32 },\n    90: { 0.995: 59.20, 0.99: 61.75, 0.975: 65.65, 0.95: 69.13, 0.9: 73.29, 0.5: 89.33, 0.1: 107.57, 0.05: 113.14, 0.025: 118.14, 0.01: 124.12, 0.005: 128.30 },\n    100: { 0.995: 67.33, 0.99: 70.06, 0.975: 74.22, 0.95: 77.93, 0.9: 82.36, 0.5: 99.33, 0.1: 118.50, 0.05: 124.34, 0.025: 129.56, 0.01: 135.81, 0.005: 140.17 }\n};\n\nmodule.exports = chiSquaredDistributionTable;\n",
    "'use strict';\n\nvar mean = require(21);\nvar chiSquaredDistributionTable = require(5);\n\n/**\n * The [χ2 (Chi-Squared) Goodness-of-Fit Test](http://en.wikipedia.org/wiki/Goodness_of_fit#Pearson.27s_chi-squared_test)\n * uses a measure of goodness of fit which is the sum of differences between observed and expected outcome frequencies\n * (that is, counts of observations), each squared and divided by the number of observations expected given the\n * hypothesized distribution. The resulting χ2 statistic, `chiSquared`, can be compared to the chi-squared distribution\n * to determine the goodness of fit. In order to determine the degrees of freedom of the chi-squared distribution, one\n * takes the total number of observed frequencies and subtracts the number of estimated parameters. The test statistic\n * follows, approximately, a chi-square distribution with (k − c) degrees of freedom where `k` is the number of non-empty\n * cells and `c` is the number of estimated parameters for the distribution.\n *\n * @param {Array<number>} data\n * @param {Function} distributionType a function that returns a point in a distribution:\n * for instance, binomial, bernoulli, or poisson\n * @param {number} significance\n * @returns {number} chi squared goodness of fit\n * @example\n * // Data from Poisson goodness-of-fit example 10-19 in William W. Hines & Douglas C. Montgomery,\n * // \"Probability and Statistics in Engineering and Management Science\", Wiley (1980).\n * var data1019 = [\n *     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n *     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n *     1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n *     2, 2, 2, 2, 2, 2, 2, 2, 2,\n *     3, 3, 3, 3\n * ];\n * ss.chiSquaredGoodnessOfFit(data1019, ss.poissonDistribution, 0.05)); //= false\n */\nfunction chiSquaredGoodnessOfFit(data, distributionType, significance) {\n    // Estimate from the sample data, a weighted mean.\n    var inputMean = mean(data),\n        // Calculated value of the χ2 statistic.\n        chiSquared = 0,\n        // Degrees of freedom, calculated as (number of class intervals -\n        // number of hypothesized distribution parameters estimated - 1)\n        degreesOfFreedom,\n        // Number of hypothesized distribution parameters estimated, expected to be supplied in the distribution test.\n        // Lose one degree of freedom for estimating `lambda` from the sample data.\n        c = 1,\n        // The hypothesized distribution.\n        // Generate the hypothesized distribution.\n        hypothesizedDistribution = distributionType(inputMean),\n        observedFrequencies = [],\n        expectedFrequencies = [],\n        k;\n\n    // Create an array holding a histogram from the sample data, of\n    // the form `{ value: numberOfOcurrences }`\n    for (var i = 0; i < data.length; i++) {\n        if (observedFrequencies[data[i]] === undefined) {\n            observedFrequencies[data[i]] = 0;\n        }\n        observedFrequencies[data[i]]++;\n    }\n\n    // The histogram we created might be sparse - there might be gaps\n    // between values. So we iterate through the histogram, making\n    // sure that instead of undefined, gaps have 0 values.\n    for (i = 0; i < observedFrequencies.length; i++) {\n        if (observedFrequencies[i] === undefined) {\n            observedFrequencies[i] = 0;\n        }\n    }\n\n    // Create an array holding a histogram of expected data given the\n    // sample size and hypothesized distribution.\n    for (k in hypothesizedDistribution) {\n        if (k in observedFrequencies) {\n            expectedFrequencies[k] = hypothesizedDistribution[k] * data.length;\n        }\n    }\n\n    // Working backward through the expected frequencies, collapse classes\n    // if less than three observations are expected for a class.\n    // This transformation is applied to the observed frequencies as well.\n    for (k = expectedFrequencies.length - 1; k >= 0; k--) {\n        if (expectedFrequencies[k] < 3) {\n            expectedFrequencies[k - 1] += expectedFrequencies[k];\n            expectedFrequencies.pop();\n\n            observedFrequencies[k - 1] += observedFrequencies[k];\n            observedFrequencies.pop();\n        }\n    }\n\n    // Iterate through the squared differences between observed & expected\n    // frequencies, accumulating the `chiSquared` statistic.\n    for (k = 0; k < observedFrequencies.length; k++) {\n        chiSquared += Math.pow(\n            observedFrequencies[k] - expectedFrequencies[k], 2) /\n            expectedFrequencies[k];\n    }\n\n    // Calculate degrees of freedom for this test and look it up in the\n    // `chiSquaredDistributionTable` in order to\n    // accept or reject the goodness-of-fit of the hypothesized distribution.\n    degreesOfFreedom = observedFrequencies.length - c - 1;\n    return chiSquaredDistributionTable[degreesOfFreedom][significance] < chiSquared;\n}\n\nmodule.exports = chiSquaredGoodnessOfFit;\n",
    "'use strict';\n\n/**\n * Split an array into chunks of a specified size. This function\n * has the same behavior as [PHP's array_chunk](http://php.net/manual/en/function.array-chunk.php)\n * function, and thus will insert smaller-sized chunks at the end if\n * the input size is not divisible by the chunk size.\n *\n * `sample` is expected to be an array, and `chunkSize` a number.\n * The `sample` array can contain any kind of data.\n *\n * @param {Array} sample any array of values\n * @param {number} chunkSize size of each output array\n * @returns {Array<Array>} a chunked array\n * @example\n * console.log(chunk([1, 2, 3, 4], 2)); // [[1, 2], [3, 4]]\n */\nfunction chunk(sample, chunkSize) {\n\n    // a list of result chunks, as arrays in an array\n    var output = [];\n\n    // `chunkSize` must be zero or higher - otherwise the loop below,\n    // in which we call `start += chunkSize`, will loop infinitely.\n    // So, we'll detect and return null in that case to indicate\n    // invalid input.\n    if (chunkSize <= 0) {\n        return null;\n    }\n\n    // `start` is the index at which `.slice` will start selecting\n    // new array elements\n    for (var start = 0; start < sample.length; start += chunkSize) {\n\n        // for each chunk, slice that part of the array and add it\n        // to the output. The `.slice` function does not change\n        // the original array.\n        output.push(sample.slice(start, start + chunkSize));\n    }\n    return output;\n}\n\nmodule.exports = chunk;\n",
    "'use strict';\n\nvar sortedUniqueCount = require(42),\n    numericSort = require(26);\n\n/**\n * Create a new column x row matrix.\n *\n * @private\n * @param {number} columns\n * @param {number} rows\n * @return {Array<Array<number>>} matrix\n * @example\n * makeMatrix(10, 10);\n */\nfunction makeMatrix(columns, rows) {\n    var matrix = [];\n    for (var i = 0; i < columns; i++) {\n        var column = [];\n        for (var j = 0; j < rows; j++) {\n            column.push(0);\n        }\n        matrix.push(column);\n    }\n    return matrix;\n}\n\n/**\n * Ckmeans clustering is an improvement on heuristic-based clustering\n * approaches like Jenks. The algorithm was developed in\n * [Haizhou Wang and Mingzhou Song](http://journal.r-project.org/archive/2011-2/RJournal_2011-2_Wang+Song.pdf)\n * as a [dynamic programming](https://en.wikipedia.org/wiki/Dynamic_programming) approach\n * to the problem of clustering numeric data into groups with the least\n * within-group sum-of-squared-deviations.\n *\n * Minimizing the difference within groups - what Wang & Song refer to as\n * `withinss`, or within sum-of-squares, means that groups are optimally\n * homogenous within and the data is split into representative groups.\n * This is very useful for visualization, where you may want to represent\n * a continuous variable in discrete color or style groups. This function\n * can provide groups that emphasize differences between data.\n *\n * Being a dynamic approach, this algorithm is based on two matrices that\n * store incrementally-computed values for squared deviations and backtracking\n * indexes.\n *\n * Unlike the [original implementation](https://cran.r-project.org/web/packages/Ckmeans.1d.dp/index.html),\n * this implementation does not include any code to automatically determine\n * the optimal number of clusters: this information needs to be explicitly\n * provided.\n *\n * ### References\n * _Ckmeans.1d.dp: Optimal k-means Clustering in One Dimension by Dynamic\n * Programming_ Haizhou Wang and Mingzhou Song ISSN 2073-4859\n *\n * from The R Journal Vol. 3/2, December 2011\n * @param {Array<number>} data input data, as an array of number values\n * @param {number} nClusters number of desired classes. This cannot be\n * greater than the number of values in the data array.\n * @returns {Array<Array<number>>} clustered input\n * @example\n * ckmeans([-1, 2, -1, 2, 4, 5, 6, -1, 2, -1], 3);\n * // The input, clustered into groups of similar numbers.\n * //= [[-1, -1, -1, -1], [2, 2, 2], [4, 5, 6]]);\n */\nfunction ckmeans(data, nClusters) {\n\n    if (nClusters > data.length) {\n        throw new Error('Cannot generate more classes than there are data values');\n    }\n\n    var sorted = numericSort(data),\n        // we'll use this as the maximum number of clusters\n        uniqueCount = sortedUniqueCount(sorted);\n\n    // if all of the input values are identical, there's one cluster\n    // with all of the input in it.\n    if (uniqueCount === 1) {\n        return [sorted];\n    }\n\n    // named 'D' originally\n    var matrix = makeMatrix(nClusters, sorted.length),\n        // named 'B' originally\n        backtrackMatrix = makeMatrix(nClusters, sorted.length);\n\n    // This is a dynamic programming way to solve the problem of minimizing\n    // within-cluster sum of squares. It's similar to linear regression\n    // in this way, and this calculation incrementally computes the\n    // sum of squares that are later read.\n\n    // The outer loop iterates through clusters, from 0 to nClusters.\n    for (var cluster = 0; cluster < nClusters; cluster++) {\n\n        // At the start of each loop, the mean starts as the first element\n        var firstClusterMean = sorted[0];\n\n        for (var sortedIdx = Math.max(cluster, 1);\n             sortedIdx < sorted.length;\n             sortedIdx++) {\n\n            if (cluster === 0) {\n\n                // Increase the running sum of squares calculation by this\n                // new value\n                var squaredDifference = Math.pow(\n                    sorted[sortedIdx] - firstClusterMean, 2);\n                matrix[cluster][sortedIdx] = matrix[cluster][sortedIdx - 1] +\n                    (sortedIdx / (sortedIdx + 1)) * squaredDifference;\n\n                // We're computing a running mean by taking the previous\n                // mean value, multiplying it by the number of elements\n                // seen so far, and then dividing it by the number of\n                // elements total.\n                var newSum = sortedIdx * firstClusterMean + sorted[sortedIdx];\n                firstClusterMean = newSum / (sortedIdx + 1);\n\n            } else {\n\n                var sumSquaredDistances = 0,\n                    meanXJ = 0;\n\n                for (var j = sortedIdx; j >= cluster; j--) {\n\n                    sumSquaredDistances += (sortedIdx - j) /\n                        (sortedIdx - j + 1) *\n                        Math.pow(sorted[j] - meanXJ, 2);\n\n                    meanXJ = (sorted[j] + (sortedIdx - j) * meanXJ) /\n                        (sortedIdx - j + 1);\n\n                    if (j === sortedIdx) {\n                        matrix[cluster][sortedIdx] = sumSquaredDistances;\n                        backtrackMatrix[cluster][sortedIdx] = j;\n                        if (j > 0) {\n                            matrix[cluster][sortedIdx] += matrix[cluster - 1][j - 1];\n                        }\n                    } else {\n                        if (j === 0) {\n                            if (sumSquaredDistances <= matrix[cluster][sortedIdx]) {\n                                matrix[cluster][sortedIdx] = sumSquaredDistances;\n                                backtrackMatrix[cluster][sortedIdx] = j;\n                            }\n                        } else if (sumSquaredDistances + matrix[cluster - 1][j - 1] < matrix[cluster][sortedIdx]) {\n                            matrix[cluster][sortedIdx] = sumSquaredDistances + matrix[cluster - 1][j - 1];\n                            backtrackMatrix[cluster][sortedIdx] = j;\n                        }\n                    }\n                }\n            }\n        }\n    }\n\n    // The real work of Ckmeans clustering happens in the matrix generation:\n    // the generated matrices encode all possible clustering combinations, and\n    // once they're generated we can solve for the best clustering groups\n    // very quickly.\n    var clusters = [],\n        clusterRight = backtrackMatrix[0].length - 1;\n\n    // Backtrack the clusters from the dynamic programming matrix. This\n    // starts at the bottom-right corner of the matrix (if the top-left is 0, 0),\n    // and moves the cluster target with the loop.\n    for (cluster = backtrackMatrix.length - 1; cluster >= 0; cluster--) {\n\n        var clusterLeft = backtrackMatrix[cluster][clusterRight];\n\n        // fill the cluster from the sorted input by taking a slice of the\n        // array. the backtrack matrix makes this easy - it stores the\n        // indexes where the cluster should start and end.\n        clusters[cluster] = sorted.slice(clusterLeft, clusterRight + 1);\n\n        if (cluster > 0) {\n            clusterRight = clusterLeft - 1;\n        }\n    }\n\n    return clusters;\n}\n\nmodule.exports = ckmeans;\n",
    "'use strict';\n\nvar standardNormalTable = require(44);\n\n/**\n * **[Cumulative Standard Normal Probability](http://en.wikipedia.org/wiki/Standard_normal_table)**\n *\n * Since probability tables cannot be\n * printed for every normal distribution, as there are an infinite variety\n * of normal distributions, it is common practice to convert a normal to a\n * standard normal and then use the standard normal table to find probabilities.\n *\n * You can use `.5 + .5 * errorFunction(x / Math.sqrt(2))` to calculate the probability\n * instead of looking it up in a table.\n *\n * @param {number} z\n * @returns {number} cumulative standard normal probability\n */\nfunction cumulativeStdNormalProbability(z) {\n\n    // Calculate the position of this value.\n    var absZ = Math.abs(z),\n        // Each row begins with a different\n        // significant digit: 0.5, 0.6, 0.7, and so on. Each value in the table\n        // corresponds to a range of 0.01 in the input values, so the value is\n        // multiplied by 100.\n        index = Math.min(Math.round(absZ * 100), standardNormalTable.length - 1);\n\n    // The index we calculate must be in the table as a positive value,\n    // but we still pay attention to whether the input is positive\n    // or negative, and flip the output value as a last step.\n    if (z >= 0) {\n        return standardNormalTable[index];\n    } else {\n        // due to floating-point arithmetic, values in the table with\n        // 4 significant figures can nevertheless end up as repeating\n        // fractions when they're computed here.\n        return +(1 - standardNormalTable[index]).toFixed(4);\n    }\n}\n\nmodule.exports = cumulativeStdNormalProbability;\n",
    "'use strict';\n\n/**\n * We use `ε`, epsilon, as a stopping criterion when we want to iterate\n * until we're \"close enough\". Epsilon is a very small number: for\n * simple statistics, that number is **0.0001**\n *\n * This is used in calculations like the binomialDistribution, in which\n * the process of finding a value is [iterative](https://en.wikipedia.org/wiki/Iterative_method):\n * it progresses until it is close enough.\n *\n * Below is an example of using epsilon in [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent),\n * where we're trying to find a local minimum of a function's derivative,\n * given by the `fDerivative` method.\n *\n * @example\n * // From calculation, we expect that the local minimum occurs at x=9/4\n * var x_old = 0;\n * // The algorithm starts at x=6\n * var x_new = 6;\n * var stepSize = 0.01;\n *\n * function fDerivative(x) {\n *   return 4 * Math.pow(x, 3) - 9 * Math.pow(x, 2);\n * }\n *\n * // The loop runs until the difference between the previous\n * // value and the current value is smaller than epsilon - a rough\n * // meaure of 'close enough'\n * while (Math.abs(x_new - x_old) > ss.epsilon) {\n *   x_old = x_new;\n *   x_new = x_old - stepSize * fDerivative(x_old);\n * }\n *\n * console.log('Local minimum occurs at', x_new);\n */\nvar epsilon = 0.0001;\n\nmodule.exports = epsilon;\n",
    "'use strict';\n\n/**\n * **[Gaussian error function](http://en.wikipedia.org/wiki/Error_function)**\n *\n * The `errorFunction(x/(sd * Math.sqrt(2)))` is the probability that a value in a\n * normal distribution with standard deviation sd is within x of the mean.\n *\n * This function returns a numerical approximation to the exact value.\n *\n * @param {number} x input\n * @return {number} error estimation\n * @example\n * errorFunction(1); //= 0.8427\n */\nfunction errorFunction(x) {\n    var t = 1 / (1 + 0.5 * Math.abs(x));\n    var tau = t * Math.exp(-Math.pow(x, 2) -\n        1.26551223 +\n        1.00002368 * t +\n        0.37409196 * Math.pow(t, 2) +\n        0.09678418 * Math.pow(t, 3) -\n        0.18628806 * Math.pow(t, 4) +\n        0.27886807 * Math.pow(t, 5) -\n        1.13520398 * Math.pow(t, 6) +\n        1.48851587 * Math.pow(t, 7) -\n        0.82215223 * Math.pow(t, 8) +\n        0.17087277 * Math.pow(t, 9));\n    if (x >= 0) {\n        return 1 - tau;\n    } else {\n        return tau - 1;\n    }\n}\n\nmodule.exports = errorFunction;\n",
    "'use strict';\n\n/**\n * A [Factorial](https://en.wikipedia.org/wiki/Factorial), usually written n!, is the product of all positive\n * integers less than or equal to n. Often factorial is implemented\n * recursively, but this iterative approach is significantly faster\n * and simpler.\n *\n * @param {number} n input\n * @returns {number} factorial: n!\n * @example\n * console.log(factorial(5)); // 120\n */\nfunction factorial(n) {\n\n    // factorial is mathematically undefined for negative numbers\n    if (n < 0 ) { return null; }\n\n    // typically you'll expand the factorial function going down, like\n    // 5! = 5 * 4 * 3 * 2 * 1. This is going in the opposite direction,\n    // counting from 2 up to the number in question, and since anything\n    // multiplied by 1 is itself, the loop only needs to start at 2.\n    var accumulator = 1;\n    for (var i = 2; i <= n; i++) {\n        // for each number up to and including the number `n`, multiply\n        // the accumulator my that number.\n        accumulator *= i;\n    }\n    return accumulator;\n}\n\nmodule.exports = factorial;\n",
    "'use strict';\n\n/**\n * The [Geometric Mean](https://en.wikipedia.org/wiki/Geometric_mean) is\n * a mean function that is more useful for numbers in different\n * ranges.\n *\n * This is the nth root of the input numbers multiplied by each other.\n *\n * The geometric mean is often useful for\n * **[proportional growth](https://en.wikipedia.org/wiki/Geometric_mean#Proportional_growth)**: given\n * growth rates for multiple years, like _80%, 16.66% and 42.85%_, a simple\n * mean will incorrectly estimate an average growth rate, whereas a geometric\n * mean will correctly estimate a growth rate that, over those years,\n * will yield the same end value.\n *\n * This runs on `O(n)`, linear time in respect to the array\n *\n * @param {Array<number>} x input array\n * @returns {number} geometric mean\n * @example\n * var growthRates = [1.80, 1.166666, 1.428571];\n * var averageGrowth = geometricMean(growthRates);\n * var averageGrowthRates = [averageGrowth, averageGrowth, averageGrowth];\n * var startingValue = 10;\n * var startingValueMean = 10;\n * growthRates.forEach(function(rate) {\n *   startingValue *= rate;\n * });\n * averageGrowthRates.forEach(function(rate) {\n *   startingValueMean *= rate;\n * });\n * startingValueMean === startingValue;\n */\nfunction geometricMean(x) {\n    // The mean of no numbers is null\n    if (x.length === 0) { return null; }\n\n    // the starting value.\n    var value = 1;\n\n    for (var i = 0; i < x.length; i++) {\n        // the geometric mean is only valid for positive numbers\n        if (x[i] <= 0) { return null; }\n\n        // repeatedly multiply the value by each number\n        value *= x[i];\n    }\n\n    return Math.pow(value, 1 / x.length);\n}\n\nmodule.exports = geometricMean;\n",
    "'use strict';\n\n/**\n * The [Harmonic Mean](https://en.wikipedia.org/wiki/Harmonic_mean) is\n * a mean function typically used to find the average of rates.\n * This mean is calculated by taking the reciprocal of the arithmetic mean\n * of the reciprocals of the input numbers.\n *\n * This is a [measure of central tendency](https://en.wikipedia.org/wiki/Central_tendency):\n * a method of finding a typical or central value of a set of numbers.\n *\n * This runs on `O(n)`, linear time in respect to the array.\n *\n * @param {Array<number>} x input\n * @returns {number} harmonic mean\n * @example\n * ss.harmonicMean([2, 3]) //= 2.4\n */\nfunction harmonicMean(x) {\n    // The mean of no numbers is null\n    if (x.length === 0) { return null; }\n\n    var reciprocalSum = 0;\n\n    for (var i = 0; i < x.length; i++) {\n        // the harmonic mean is only valid for positive numbers\n        if (x[i] <= 0) { return null; }\n\n        reciprocalSum += 1 / x[i];\n    }\n\n    // divide n by the the reciprocal sum\n    return x.length / reciprocalSum;\n}\n\nmodule.exports = harmonicMean;\n",
    "'use strict';\n\nvar quantile = require(30);\n\n/**\n * The [Interquartile range](http://en.wikipedia.org/wiki/Interquartile_range) is\n * a measure of statistical dispersion, or how scattered, spread, or\n * concentrated a distribution is. It's computed as the difference between\n * the third quartile and first quartile.\n *\n * @param {Array<number>} sample\n * @returns {number} interquartile range: the span between lower and upper quartile,\n * 0.25 and 0.75\n * @example\n * interquartileRange([0, 1, 2, 3]); //= 2\n */\nfunction interquartileRange(sample) {\n    // We can't derive quantiles from an empty list\n    if (sample.length === 0) { return null; }\n\n    // Interquartile range is the span between the upper quartile,\n    // at `0.75`, and lower quartile, `0.25`\n    return quantile(sample, 0.75) - quantile(sample, 0.25);\n}\n\nmodule.exports = interquartileRange;\n",
    "'use strict';\n\n/**\n * The Inverse [Gaussian error function](http://en.wikipedia.org/wiki/Error_function)\n * returns a numerical approximation to the value that would have caused\n * `errorFunction()` to return x.\n *\n * @param {number} x value of error function\n * @returns {number} estimated inverted value\n */\nfunction inverseErrorFunction(x) {\n    var a = (8 * (Math.PI - 3)) / (3 * Math.PI * (4 - Math.PI));\n\n    var inv = Math.sqrt(Math.sqrt(\n        Math.pow(2 / (Math.PI * a) + Math.log(1 - x * x) / 2, 2) -\n        Math.log(1 - x * x) / a) -\n        (2 / (Math.PI * a) + Math.log(1 - x * x) / 2));\n\n    if (x >= 0) {\n        return inv;\n    } else {\n        return -inv;\n    }\n}\n\nmodule.exports = inverseErrorFunction;\n",
    "'use strict';\n\n/**\n * [Simple linear regression](http://en.wikipedia.org/wiki/Simple_linear_regression)\n * is a simple way to find a fitted line\n * between a set of coordinates. This algorithm finds the slope and y-intercept of a regression line\n * using the least sum of squares.\n *\n * @param {Array<Array<number>>} data an array of two-element of arrays,\n * like `[[0, 1], [2, 3]]`\n * @returns {Object} object containing slope and intersect of regression line\n * @example\n * linearRegression([[0, 0], [1, 1]]); // { m: 1, b: 0 }\n */\nfunction linearRegression(data) {\n\n    var m, b;\n\n    // Store data length in a local variable to reduce\n    // repeated object property lookups\n    var dataLength = data.length;\n\n    //if there's only one point, arbitrarily choose a slope of 0\n    //and a y-intercept of whatever the y of the initial point is\n    if (dataLength === 1) {\n        m = 0;\n        b = data[0][1];\n    } else {\n        // Initialize our sums and scope the `m` and `b`\n        // variables that define the line.\n        var sumX = 0, sumY = 0,\n            sumXX = 0, sumXY = 0;\n\n        // Use local variables to grab point values\n        // with minimal object property lookups\n        var point, x, y;\n\n        // Gather the sum of all x values, the sum of all\n        // y values, and the sum of x^2 and (x*y) for each\n        // value.\n        //\n        // In math notation, these would be SS_x, SS_y, SS_xx, and SS_xy\n        for (var i = 0; i < dataLength; i++) {\n            point = data[i];\n            x = point[0];\n            y = point[1];\n\n            sumX += x;\n            sumY += y;\n\n            sumXX += x * x;\n            sumXY += x * y;\n        }\n\n        // `m` is the slope of the regression line\n        m = ((dataLength * sumXY) - (sumX * sumY)) /\n            ((dataLength * sumXX) - (sumX * sumX));\n\n        // `b` is the y-intercept of the line.\n        b = (sumY / dataLength) - ((m * sumX) / dataLength);\n    }\n\n    // Return both values as an object.\n    return {\n        m: m,\n        b: b\n    };\n}\n\n\nmodule.exports = linearRegression;\n",
    "'use strict';\n\n/**\n * Given the output of `linearRegression`: an object\n * with `m` and `b` values indicating slope and intercept,\n * respectively, generate a line function that translates\n * x values into y values.\n *\n * @param {Object} mb object with `m` and `b` members, representing\n * slope and intersect of desired line\n * @returns {Function} method that computes y-value at any given\n * x-value on the line.\n * @example\n * var l = linearRegressionLine(linearRegression([[0, 0], [1, 1]]));\n * l(0) //= 0\n * l(2) //= 2\n */\nfunction linearRegressionLine(mb) {\n    // Return a function that computes a `y` value for each\n    // x value it is given, based on the values of `b` and `a`\n    // that we just computed.\n    return function(x) {\n        return mb.b + (mb.m * x);\n    };\n}\n\nmodule.exports = linearRegressionLine;\n",
    "'use strict';\n\nvar median = require(22);\n\n/**\n * The [Median Absolute Deviation](http://en.wikipedia.org/wiki/Median_absolute_deviation) is\n * a robust measure of statistical\n * dispersion. It is more resilient to outliers than the standard deviation.\n *\n * @param {Array<number>} x input array\n * @returns {number} median absolute deviation\n * @example\n * mad([1, 1, 2, 2, 4, 6, 9]); //= 1\n */\nfunction mad(x) {\n    // The mad of nothing is null\n    if (!x || x.length === 0) { return null; }\n\n    var medianValue = median(x),\n        medianAbsoluteDeviations = [];\n\n    // Make a list of absolute deviations from the median\n    for (var i = 0; i < x.length; i++) {\n        medianAbsoluteDeviations.push(Math.abs(x[i] - medianValue));\n    }\n\n    // Find the median value of that list\n    return median(medianAbsoluteDeviations);\n}\n\nmodule.exports = mad;\n",
    "'use strict';\n\n/**\n * This computes the maximum number in an array.\n *\n * This runs on `O(n)`, linear time in respect to the array\n *\n * @param {Array<number>} x input\n * @returns {number} maximum value\n * @example\n * console.log(max([1, 2, 3, 4])); // 4\n */\nfunction max(x) {\n    var value;\n    for (var i = 0; i < x.length; i++) {\n        // On the first iteration of this loop, max is\n        // undefined and is thus made the maximum element in the array\n        if (x[i] > value || value === undefined) {\n            value = x[i];\n        }\n    }\n    return value;\n}\n\nmodule.exports = max;\n",
    "'use strict';\n\nvar sum = require(45);\n\n/**\n * The mean, _also known as average_,\n * is the sum of all values over the number of values.\n * This is a [measure of central tendency](https://en.wikipedia.org/wiki/Central_tendency):\n * a method of finding a typical or central value of a set of numbers.\n *\n * This runs on `O(n)`, linear time in respect to the array\n *\n * @param {Array<number>} x input values\n * @returns {number} mean\n * @example\n * console.log(mean([0, 10])); // 5\n */\nfunction mean(x) {\n    // The mean of no numbers is null\n    if (x.length === 0) { return null; }\n\n    return sum(x) / x.length;\n}\n\nmodule.exports = mean;\n",
    "'use strict';\n\nvar numericSort = require(26);\n\n/**\n * The [median](http://en.wikipedia.org/wiki/Median) is\n * the middle number of a list. This is often a good indicator of 'the middle'\n * when there are outliers that skew the `mean()` value.\n * This is a [measure of central tendency](https://en.wikipedia.org/wiki/Central_tendency):\n * a method of finding a typical or central value of a set of numbers.\n *\n * The median isn't necessarily one of the elements in the list: the value\n * can be the average of two elements if the list has an even length\n * and the two central values are different.\n *\n * @param {Array<number>} x input\n * @returns {number} median value\n * @example\n * var incomes = [10, 2, 5, 100, 2, 1];\n * median(incomes); //= 3.5\n */\nfunction median(x) {\n    // The median of an empty list is null\n    if (x.length === 0) { return null; }\n\n    // Sorting the array makes it easy to find the center, but\n    // use `.slice()` to ensure the original array `x` is not modified\n    var sorted = numericSort(x);\n\n    // If the length of the list is odd, it's the central number\n    if (sorted.length % 2 === 1) {\n        return sorted[(sorted.length - 1) / 2];\n    // Otherwise, the median is the average of the two numbers\n    // at the center of the list\n    } else {\n        var a = sorted[sorted.length / 2 - 1];\n        var b = sorted[sorted.length / 2];\n        return (a + b) / 2;\n    }\n}\n\nmodule.exports = median;\n",
    "'use strict';\n\n/**\n * The min is the lowest number in the array. This runs on `O(n)`, linear time in respect to the array\n *\n * @param {Array<number>} x input\n * @returns {number} minimum value\n * @example\n * min([1, 5, -10, 100, 2]); // -100\n */\nfunction min(x) {\n    var value;\n    for (var i = 0; i < x.length; i++) {\n        // On the first iteration of this loop, min is\n        // undefined and is thus made the minimum element in the array\n        if (x[i] < value || value === undefined) {\n            value = x[i];\n        }\n    }\n    return value;\n}\n\nmodule.exports = min;\n",
    "'use strict';\n\n/**\n * **Mixin** simple_statistics to a single Array instance if provided\n * or the Array native object if not. This is an optional\n * feature that lets you treat simple_statistics as a native feature\n * of Javascript.\n *\n * @param {Object} ss simple statistics\n * @param {Array} [array=] a single array instance which will be augmented\n * with the extra methods. If omitted, mixin will apply to all arrays\n * by changing the global `Array.prototype`.\n * @returns {*} the extended Array, or Array.prototype if no object\n * is given.\n *\n * @example\n * var myNumbers = [1, 2, 3];\n * mixin(ss, myNumbers);\n * console.log(myNumbers.sum()); // 6\n */\nfunction mixin(ss, array) {\n    var support = !!(Object.defineProperty && Object.defineProperties);\n    // Coverage testing will never test this error.\n    /* istanbul ignore next */\n    if (!support) {\n        throw new Error('without defineProperty, simple-statistics cannot be mixed in');\n    }\n\n    // only methods which work on basic arrays in a single step\n    // are supported\n    var arrayMethods = ['median', 'standardDeviation', 'sum',\n        'sampleSkewness',\n        'mean', 'min', 'max', 'quantile', 'geometricMean',\n        'harmonicMean', 'root_mean_square'];\n\n    // create a closure with a method name so that a reference\n    // like `arrayMethods[i]` doesn't follow the loop increment\n    function wrap(method) {\n        return function() {\n            // cast any arguments into an array, since they're\n            // natively objects\n            var args = Array.prototype.slice.apply(arguments);\n            // make the first argument the array itself\n            args.unshift(this);\n            // return the result of the ss method\n            return ss[method].apply(ss, args);\n        };\n    }\n\n    // select object to extend\n    var extending;\n    if (array) {\n        // create a shallow copy of the array so that our internal\n        // operations do not change it by reference\n        extending = array.slice();\n    } else {\n        extending = Array.prototype;\n    }\n\n    // for each array function, define a function that gets\n    // the array as the first argument.\n    // We use [defineProperty](https://developer.mozilla.org/en-US/docs/JavaScript/Reference/Global_Objects/Object/defineProperty)\n    // because it allows these properties to be non-enumerable:\n    // `for (var in x)` loops will not run into problems with this\n    // implementation.\n    for (var i = 0; i < arrayMethods.length; i++) {\n        Object.defineProperty(extending, arrayMethods[i], {\n            value: wrap(arrayMethods[i]),\n            configurable: true,\n            enumerable: false,\n            writable: true\n        });\n    }\n\n    return extending;\n}\n\nmodule.exports = mixin;\n",
    "'use strict';\n\nvar numericSort = require(26);\n\n/**\n * The [mode](http://bit.ly/W5K4Yt) is the number that appears in a list the highest number of times.\n * There can be multiple modes in a list: in the event of a tie, this\n * algorithm will return the most recently seen mode.\n *\n * This is a [measure of central tendency](https://en.wikipedia.org/wiki/Central_tendency):\n * a method of finding a typical or central value of a set of numbers.\n *\n * This runs on `O(n)`, linear time in respect to the array.\n *\n * @param {Array<number>} x input\n * @returns {number} mode\n * @example\n * mode([0, 0, 1]); //= 0\n */\nfunction mode(x) {\n\n    // Handle edge cases:\n    // The median of an empty list is null\n    if (x.length === 0) { return null; }\n    else if (x.length === 1) { return x[0]; }\n\n    // Sorting the array lets us iterate through it below and be sure\n    // that every time we see a new number it's new and we'll never\n    // see the same number twice\n    var sorted = numericSort(x);\n\n    // This assumes it is dealing with an array of size > 1, since size\n    // 0 and 1 are handled immediately. Hence it starts at index 1 in the\n    // array.\n    var last = sorted[0],\n        // store the mode as we find new modes\n        value,\n        // store how many times we've seen the mode\n        maxSeen = 0,\n        // how many times the current candidate for the mode\n        // has been seen\n        seenThis = 1;\n\n    // end at sorted.length + 1 to fix the case in which the mode is\n    // the highest number that occurs in the sequence. the last iteration\n    // compares sorted[i], which is undefined, to the highest number\n    // in the series\n    for (var i = 1; i < sorted.length + 1; i++) {\n        // we're seeing a new number pass by\n        if (sorted[i] !== last) {\n            // the last number is the new mode since we saw it more\n            // often than the old one\n            if (seenThis > maxSeen) {\n                maxSeen = seenThis;\n                value = last;\n            }\n            seenThis = 1;\n            last = sorted[i];\n        // if this isn't a new number, it's one more occurrence of\n        // the potential mode\n        } else { seenThis++; }\n    }\n    return value;\n}\n\nmodule.exports = mode;\n",
    "'use strict';\n\n/**\n * Sort an array of numbers by their numeric value, ensuring that the\n * array is not changed in place.\n *\n * This is necessary because the default behavior of .sort\n * in JavaScript is to sort arrays as string values\n *\n *     [1, 10, 12, 102, 20].sort()\n *     // output\n *     [1, 10, 102, 12, 20]\n *\n * @param {Array<number>} array input array\n * @return {Array<number>} sorted array\n * @private\n * @example\n * numericSort([3, 2, 1]) // [1, 2, 3]\n */\nfunction numericSort(array) {\n    return array\n        // ensure the array is changed in-place\n        .slice()\n        // comparator function that treats input as numeric\n        .sort(function(a, b) {\n            return a - b;\n        });\n}\n\nmodule.exports = numericSort;\n",
    "'use strict';\n\n/**\n * This is a single-layer [Perceptron Classifier](http://en.wikipedia.org/wiki/Perceptron) that takes\n * arrays of numbers and predicts whether they should be classified\n * as either 0 or 1 (negative or positive examples).\n * @class\n * @example\n * // Create the model\n * var p = new PerceptronModel();\n * // Train the model with input with a diagonal boundary.\n * for (var i = 0; i < 5; i++) {\n *     p.train([1, 1], 1);\n *     p.train([0, 1], 0);\n *     p.train([1, 0], 0);\n *     p.train([0, 0], 0);\n * }\n * p.predict([0, 0]); // 0\n * p.predict([0, 1]); // 0\n * p.predict([1, 0]); // 0\n * p.predict([1, 1]); // 1\n */\nfunction PerceptronModel() {\n    // The weights, or coefficients of the model;\n    // weights are only populated when training with data.\n    this.weights = [];\n    // The bias term, or intercept; it is also a weight but\n    // it's stored separately for convenience as it is always\n    // multiplied by one.\n    this.bias = 0;\n}\n\n/**\n * **Predict**: Use an array of features with the weight array and bias\n * to predict whether an example is labeled 0 or 1.\n *\n * @param {Array<number>} features an array of features as numbers\n * @returns {number} 1 if the score is over 0, otherwise 0\n */\nPerceptronModel.prototype.predict = function(features) {\n\n    // Only predict if previously trained\n    // on the same size feature array(s).\n    if (features.length !== this.weights.length) { return null; }\n\n    // Calculate the sum of features times weights,\n    // with the bias added (implicitly times one).\n    var score = 0;\n    for (var i = 0; i < this.weights.length; i++) {\n        score += this.weights[i] * features[i];\n    }\n    score += this.bias;\n\n    // Classify as 1 if the score is over 0, otherwise 0.\n    if (score > 0) {\n        return 1;\n    } else {\n        return 0;\n    }\n};\n\n/**\n * **Train** the classifier with a new example, which is\n * a numeric array of features and a 0 or 1 label.\n *\n * @param {Array<number>} features an array of features as numbers\n * @param {number} label either 0 or 1\n * @returns {PerceptronModel} this\n */\nPerceptronModel.prototype.train = function(features, label) {\n    // Require that only labels of 0 or 1 are considered.\n    if (label !== 0 && label !== 1) { return null; }\n    // The length of the feature array determines\n    // the length of the weight array.\n    // The perceptron will continue learning as long as\n    // it keeps seeing feature arrays of the same length.\n    // When it sees a new data shape, it initializes.\n    if (features.length !== this.weights.length) {\n        this.weights = features;\n        this.bias = 1;\n    }\n    // Make a prediction based on current weights.\n    var prediction = this.predict(features);\n    // Update the weights if the prediction is wrong.\n    if (prediction !== label) {\n        var gradient = label - prediction;\n        for (var i = 0; i < this.weights.length; i++) {\n            this.weights[i] += gradient * features[i];\n        }\n        this.bias += gradient;\n    }\n    return this;\n};\n\nmodule.exports = PerceptronModel;\n",
    "'use strict';\n\nvar epsilon = require(10);\nvar factorial = require(12);\n\n/**\n * The [Poisson Distribution](http://en.wikipedia.org/wiki/Poisson_distribution)\n * is a discrete probability distribution that expresses the probability\n * of a given number of events occurring in a fixed interval of time\n * and/or space if these events occur with a known average rate and\n * independently of the time since the last event.\n *\n * The Poisson Distribution is characterized by the strictly positive\n * mean arrival or occurrence rate, `λ`.\n *\n * @param {number} lambda location poisson distribution\n * @returns {number} value of poisson distribution at that point\n */\nfunction poissonDistribution(lambda) {\n    // Check that lambda is strictly positive\n    if (lambda <= 0) { return null; }\n\n    // our current place in the distribution\n    var x = 0,\n        // and we keep track of the current cumulative probability, in\n        // order to know when to stop calculating chances.\n        cumulativeProbability = 0,\n        // the calculated cells to be returned\n        cells = {};\n\n    // This algorithm iterates through each potential outcome,\n    // until the `cumulativeProbability` is very close to 1, at\n    // which point we've defined the vast majority of outcomes\n    do {\n        // a [probability mass function](https://en.wikipedia.org/wiki/Probability_mass_function)\n        cells[x] = (Math.pow(Math.E, -lambda) * Math.pow(lambda, x)) / factorial(x);\n        cumulativeProbability += cells[x];\n        x++;\n    // when the cumulativeProbability is nearly 1, we've calculated\n    // the useful range of this distribution\n    } while (cumulativeProbability < 1 - epsilon);\n\n    return cells;\n}\n\nmodule.exports = poissonDistribution;\n",
    "'use strict';\n\nvar epsilon = require(10);\nvar inverseErrorFunction = require(16);\n\n/**\n * The [Probit](http://en.wikipedia.org/wiki/Probit)\n * is the inverse of cumulativeStdNormalProbability(),\n * and is also known as the normal quantile function.\n *\n * It returns the number of standard deviations from the mean\n * where the p'th quantile of values can be found in a normal distribution.\n * So, for example, probit(0.5 + 0.6827/2) ≈ 1 because 68.27% of values are\n * normally found within 1 standard deviation above or below the mean.\n *\n * @param {number} p\n * @returns {number} probit\n */\nfunction probit(p) {\n    if (p === 0) {\n        p = epsilon;\n    } else if (p >= 1) {\n        p = 1 - epsilon;\n    }\n    return Math.sqrt(2) * inverseErrorFunction(2 * p - 1);\n}\n\nmodule.exports = probit;\n",
    "'use strict';\n\nvar quantileSorted = require(31);\nvar numericSort = require(26);\n\n/**\n * The [quantile](https://en.wikipedia.org/wiki/Quantile):\n * this is a population quantile, since we assume to know the entire\n * dataset in this library. This is an implementation of the\n * [Quantiles of a Population](http://en.wikipedia.org/wiki/Quantile#Quantiles_of_a_population)\n * algorithm from wikipedia.\n *\n * Sample is a one-dimensional array of numbers,\n * and p is either a decimal number from 0 to 1 or an array of decimal\n * numbers from 0 to 1.\n * In terms of a k/q quantile, p = k/q - it's just dealing with fractions or dealing\n * with decimal values.\n * When p is an array, the result of the function is also an array containing the appropriate\n * quantiles in input order\n *\n * @param {Array<number>} sample a sample from the population\n * @param {number} p the desired quantile, as a number between 0 and 1\n * @returns {number} quantile\n * @example\n * var data = [3, 6, 7, 8, 8, 9, 10, 13, 15, 16, 20];\n * quantile(data, 1); //= max(data);\n * quantile(data, 0); //= min(data);\n * quantile(data, 0.5); //= 9\n */\nfunction quantile(sample, p) {\n\n    // We can't derive quantiles from an empty list\n    if (sample.length === 0) { return null; }\n\n    // Sort a copy of the array. We'll need a sorted array to index\n    // the values in sorted order.\n    var sorted = numericSort(sample);\n\n    if (p.length) {\n        // Initialize the result array\n        var results = [];\n        // For each requested quantile\n        for (var i = 0; i < p.length; i++) {\n            results[i] = quantileSorted(sorted, p[i]);\n        }\n        return results;\n    } else {\n        return quantileSorted(sorted, p);\n    }\n}\n\nmodule.exports = quantile;\n",
    "'use strict';\n\n/**\n * This is the internal implementation of quantiles: when you know\n * that the order is sorted, you don't need to re-sort it, and the computations\n * are faster.\n *\n * @param {Array<number>} sample input data\n * @param {number} p desired quantile: a number between 0 to 1, inclusive\n * @returns {number} quantile value\n * @example\n * var data = [3, 6, 7, 8, 8, 9, 10, 13, 15, 16, 20];\n * quantileSorted(data, 1); //= max(data);\n * quantileSorted(data, 0); //= min(data);\n * quantileSorted(data, 0.5); //= 9\n */\nfunction quantileSorted(sample, p) {\n    var idx = sample.length * p;\n    if (p < 0 || p > 1) {\n        return null;\n    } else if (p === 1) {\n        // If p is 1, directly return the last element\n        return sample[sample.length - 1];\n    } else if (p === 0) {\n        // If p is 0, directly return the first element\n        return sample[0];\n    } else if (idx % 1 !== 0) {\n        // If p is not integer, return the next element in array\n        return sample[Math.ceil(idx) - 1];\n    } else if (sample.length % 2 === 0) {\n        // If the list has even-length, we'll take the average of this number\n        // and the next value, if there is one\n        return (sample[idx - 1] + sample[idx]) / 2;\n    } else {\n        // Finally, in the simple case of an integer value\n        // with an odd-length list, return the sample value at the index.\n        return sample[idx];\n    }\n}\n\nmodule.exports = quantileSorted;\n",
    "'use strict';\n\n/**\n * The [R Squared](http://en.wikipedia.org/wiki/Coefficient_of_determination)\n * value of data compared with a function `f`\n * is the sum of the squared differences between the prediction\n * and the actual value.\n *\n * @param {Array<Array<number>>} data input data: this should be doubly-nested\n * @param {Function} func function called on `[i][0]` values within the dataset\n * @returns {number} r-squared value\n * @example\n * var samples = [[0, 0], [1, 1]];\n * var regressionLine = linearRegressionLine(linearRegression(samples));\n * rSquared(samples, regressionLine); //= 1 this line is a perfect fit\n */\nfunction rSquared(data, func) {\n    if (data.length < 2) { return 1; }\n\n    // Compute the average y value for the actual\n    // data set in order to compute the\n    // _total sum of squares_\n    var sum = 0, average;\n    for (var i = 0; i < data.length; i++) {\n        sum += data[i][1];\n    }\n    average = sum / data.length;\n\n    // Compute the total sum of squares - the\n    // squared difference between each point\n    // and the average of all points.\n    var sumOfSquares = 0;\n    for (var j = 0; j < data.length; j++) {\n        sumOfSquares += Math.pow(average - data[j][1], 2);\n    }\n\n    // Finally estimate the error: the squared\n    // difference between the estimate and the actual data\n    // value at each point.\n    var err = 0;\n    for (var k = 0; k < data.length; k++) {\n        err += Math.pow(data[k][1] - func(data[k][0]), 2);\n    }\n\n    // As the error grows larger, its ratio to the\n    // sum of squares increases and the r squared\n    // value grows lower.\n    return 1 - err / sumOfSquares;\n}\n\nmodule.exports = rSquared;\n",
    "'use strict';\n\n/**\n * The Root Mean Square (RMS) is\n * a mean function used as a measure of the magnitude of a set\n * of numbers, regardless of their sign.\n * This is the square root of the mean of the squares of the\n * input numbers.\n * This runs on `O(n)`, linear time in respect to the array\n *\n * @param {Array<number>} x input\n * @returns {number} root mean square\n * @example\n * rootMeanSquare([-1, 1, -1, 1]); //= 1\n */\nfunction rootMeanSquare(x) {\n    if (x.length === 0) { return null; }\n\n    var sumOfSquares = 0;\n    for (var i = 0; i < x.length; i++) {\n        sumOfSquares += Math.pow(x[i], 2);\n    }\n\n    return Math.sqrt(sumOfSquares / x.length);\n}\n\nmodule.exports = rootMeanSquare;\n",
    "'use strict';\n\nvar shuffle = require(40);\n\n/**\n * Create a [simple random sample](http://en.wikipedia.org/wiki/Simple_random_sample)\n * from a given array of `n` elements.\n *\n * The sampled values will be in any order, not necessarily the order\n * they appear in the input.\n *\n * @param {Array} array input array. can contain any type\n * @param {number} n count of how many elements to take\n * @param {Function} [randomSource=Math.random] an optional source of entropy\n * instead of Math.random\n * @return {Array} subset of n elements in original array\n * @example\n * var values = [1, 2, 4, 5, 6, 7, 8, 9];\n * sample(values, 3); // returns 3 random values, like [2, 5, 8];\n */\nfunction sample(array, n, randomSource) {\n    // shuffle the original array using a fisher-yates shuffle\n    var shuffled = shuffle(array, randomSource);\n\n    // and then return a subset of it - the first `n` elements.\n    return shuffled.slice(0, n);\n}\n\nmodule.exports = sample;\n",
    "'use strict';\n\nvar sampleCovariance = require(36);\nvar sampleStandardDeviation = require(38);\n\n/**\n * The [correlation](http://en.wikipedia.org/wiki/Correlation_and_dependence) is\n * a measure of how correlated two datasets are, between -1 and 1\n *\n * @param {Array<number>} x first input\n * @param {Array<number>} y second input\n * @returns {number} sample correlation\n * @example\n * var a = [1, 2, 3, 4, 5, 6];\n * var b = [2, 2, 3, 4, 5, 60];\n * sampleCorrelation(a, b); //= 0.691\n */\nfunction sampleCorrelation(x, y) {\n    var cov = sampleCovariance(x, y),\n        xstd = sampleStandardDeviation(x),\n        ystd = sampleStandardDeviation(y);\n\n    if (cov === null || xstd === null || ystd === null) {\n        return null;\n    }\n\n    return cov / xstd / ystd;\n}\n\nmodule.exports = sampleCorrelation;\n",
    "'use strict';\n\nvar mean = require(21);\n\n/**\n * [Sample covariance](https://en.wikipedia.org/wiki/Sample_mean_and_sampleCovariance) of two datasets:\n * how much do the two datasets move together?\n * x and y are two datasets, represented as arrays of numbers.\n *\n * @param {Array<number>} x first input\n * @param {Array<number>} y second input\n * @returns {number} sample covariance\n * @example\n * var x = [1, 2, 3, 4, 5, 6];\n * var y = [6, 5, 4, 3, 2, 1];\n * sampleCovariance(x, y); //= -3.5\n */\nfunction sampleCovariance(x, y) {\n\n    // The two datasets must have the same length which must be more than 1\n    if (x.length <= 1 || x.length !== y.length) {\n        return null;\n    }\n\n    // determine the mean of each dataset so that we can judge each\n    // value of the dataset fairly as the difference from the mean. this\n    // way, if one dataset is [1, 2, 3] and [2, 3, 4], their covariance\n    // does not suffer because of the difference in absolute values\n    var xmean = mean(x),\n        ymean = mean(y),\n        sum = 0;\n\n    // for each pair of values, the covariance increases when their\n    // difference from the mean is associated - if both are well above\n    // or if both are well below\n    // the mean, the covariance increases significantly.\n    for (var i = 0; i < x.length; i++) {\n        sum += (x[i] - xmean) * (y[i] - ymean);\n    }\n\n    // this is Bessels' Correction: an adjustment made to sample statistics\n    // that allows for the reduced degree of freedom entailed in calculating\n    // values from samples rather than complete populations.\n    var besselsCorrection = x.length - 1;\n\n    // the covariance is weighted by the length of the datasets.\n    return sum / besselsCorrection;\n}\n\nmodule.exports = sampleCovariance;\n",
    "'use strict';\n\nvar sumNthPowerDeviations = require(46);\nvar sampleStandardDeviation = require(38);\n\n/**\n * [Skewness](http://en.wikipedia.org/wiki/Skewness) is\n * a measure of the extent to which a probability distribution of a\n * real-valued random variable \"leans\" to one side of the mean.\n * The skewness value can be positive or negative, or even undefined.\n *\n * Implementation is based on the adjusted Fisher-Pearson standardized\n * moment coefficient, which is the version found in Excel and several\n * statistical packages including Minitab, SAS and SPSS.\n *\n * @param {Array<number>} x input\n * @returns {number} sample skewness\n * @example\n * var data = [2, 4, 6, 3, 1];\n * sampleSkewness(data); //= 0.5901286564\n */\nfunction sampleSkewness(x) {\n    // The skewness of less than three arguments is null\n    if (x.length < 3) { return null; }\n\n    var n = x.length,\n        cubedS = Math.pow(sampleStandardDeviation(x), 3),\n        sumCubedDeviations = sumNthPowerDeviations(x, 3);\n\n    return n * sumCubedDeviations / ((n - 1) * (n - 2) * cubedS);\n}\n\nmodule.exports = sampleSkewness;\n",
    "'use strict';\n\nvar sampleVariance = require(39);\n\n/**\n * The [standard deviation](http://en.wikipedia.org/wiki/Standard_deviation)\n * is the square root of the variance.\n *\n * @param {Array<number>} x input array\n * @returns {number} sample standard deviation\n * @example\n * ss.sampleStandardDeviation([2, 4, 4, 4, 5, 5, 7, 9]);\n * //= 2.138\n */\nfunction sampleStandardDeviation(x) {\n    // The standard deviation of no numbers is null\n    if (x.length <= 1) { return null; }\n\n    return Math.sqrt(sampleVariance(x));\n}\n\nmodule.exports = sampleStandardDeviation;\n",
    "'use strict';\n\nvar sumNthPowerDeviations = require(46);\n\n/*\n * The [sample variance](https://en.wikipedia.org/wiki/Variance#Sample_variance)\n * is the sum of squared deviations from the mean. The sample variance\n * is distinguished from the variance by the usage of [Bessel's Correction](https://en.wikipedia.org/wiki/Bessel's_correction):\n * instead of dividing the sum of squared deviations by the length of the input,\n * it is divided by the length minus one. This corrects the bias in estimating\n * a value from a set that you don't know if full.\n *\n * References:\n * * [Wolfram MathWorld on Sample Variance](http://mathworld.wolfram.com/SampleVariance.html)\n *\n * @param {Array<number>} x input array\n * @return {number} sample variance\n * @example\n * sampleVariance([1, 2, 3, 4, 5]); //= 2.5\n */\nfunction sampleVariance(x) {\n    // The variance of no numbers is null\n    if (x.length <= 1) { return null; }\n\n    var sumSquaredDeviationsValue = sumNthPowerDeviations(x, 2);\n\n    // this is Bessels' Correction: an adjustment made to sample statistics\n    // that allows for the reduced degree of freedom entailed in calculating\n    // values from samples rather than complete populations.\n    var besselsCorrection = x.length - 1;\n\n    // Find the mean value of that list\n    return sumSquaredDeviationsValue / besselsCorrection;\n}\n\nmodule.exports = sampleVariance;\n",
    "'use strict';\n\nvar shuffleInPlace = require(41);\n\n/*\n * A [Fisher-Yates shuffle](http://en.wikipedia.org/wiki/Fisher%E2%80%93Yates_shuffle)\n * is a fast way to create a random permutation of a finite set. This is\n * a function around `shuffle_in_place` that adds the guarantee that\n * it will not modify its input.\n *\n * @param {Array} sample an array of any kind of element\n * @param {Function} [randomSource=Math.random] an optional entropy source\n * @return {Array} shuffled version of input\n * @example\n * var shuffled = shuffle([1, 2, 3, 4]);\n * shuffled; // = [2, 3, 1, 4] or any other random permutation\n */\nfunction shuffle(sample, randomSource) {\n    // slice the original array so that it is not modified\n    sample = sample.slice();\n\n    // and then shuffle that shallow-copied array, in place\n    return shuffleInPlace(sample.slice(), randomSource);\n}\n\nmodule.exports = shuffle;\n",
    "'use strict';\n\n/*\n * A [Fisher-Yates shuffle](http://en.wikipedia.org/wiki/Fisher%E2%80%93Yates_shuffle)\n * in-place - which means that it **will change the order of the original\n * array by reference**.\n *\n * This is an algorithm that generates a random [permutation](https://en.wikipedia.org/wiki/Permutation)\n * of a set.\n *\n * @param {Array} sample input array\n * @param {Function} [randomSource=Math.random] an optional source of entropy\n * @returns {Array} sample\n * @example\n * var sample = [1, 2, 3, 4];\n * shuffleInPlace(sample);\n * // sample is shuffled to a value like [2, 1, 4, 3]\n */\nfunction shuffleInPlace(sample, randomSource) {\n\n    // a custom random number source can be provided if you want to use\n    // a fixed seed or another random number generator, like\n    // [random-js](https://www.npmjs.org/package/random-js)\n    randomSource = randomSource || Math.random;\n\n    // store the current length of the sample to determine\n    // when no elements remain to shuffle.\n    var length = sample.length;\n\n    // temporary is used to hold an item when it is being\n    // swapped between indices.\n    var temporary;\n\n    // The index to swap at each stage.\n    var index;\n\n    // While there are still items to shuffle\n    while (length > 0) {\n        // chose a random index within the subset of the array\n        // that is not yet shuffled\n        index = Math.floor(randomSource() * length--);\n\n        // store the value that we'll move temporarily\n        temporary = sample[length];\n\n        // swap the value at `sample[length]` with `sample[index]`\n        sample[length] = sample[index];\n        sample[index] = temporary;\n    }\n\n    return sample;\n}\n\nmodule.exports = shuffleInPlace;\n",
    "'use strict';\n\n/**\n * For a sorted input, counting the number of unique values\n * is possible in constant time and constant memory. This is\n * a simple implementation of the algorithm.\n *\n * Values are compared with `===`, so objects and non-primitive objects\n * are not handled in any special way.\n *\n * @param {Array} input an array of primitive values.\n * @returns {number} count of unique values\n * @example\n * sortedUniqueCount([1, 2, 3]); // 3\n * sortedUniqueCount([1, 1, 1]); // 1\n */\nfunction sortedUniqueCount(input) {\n    var uniqueValueCount = 0,\n        lastSeenValue;\n    for (var i = 0; i < input.length; i++) {\n        if (i === 0 || input[i] !== lastSeenValue) {\n            lastSeenValue = input[i];\n            uniqueValueCount++;\n        }\n    }\n    return uniqueValueCount;\n}\n\nmodule.exports = sortedUniqueCount;\n",
    "'use strict';\n\nvar variance = require(49);\n\n/**\n * The [standard deviation](http://en.wikipedia.org/wiki/Standard_deviation)\n * is the square root of the variance. It's useful for measuring the amount\n * of variation or dispersion in a set of values.\n *\n * Standard deviation is only appropriate for full-population knowledge: for\n * samples of a population, {@link sampleStandardDeviation} is\n * more appropriate.\n *\n * @param {Array<number>} x input\n * @returns {number} standard deviation\n * @example\n * var scores = [2, 4, 4, 4, 5, 5, 7, 9];\n * variance(scores); //= 4\n * standardDeviation(scores); //= 2\n */\nfunction standardDeviation(x) {\n    // The standard deviation of no numbers is null\n    if (x.length === 0) { return null; }\n\n    return Math.sqrt(variance(x));\n}\n\nmodule.exports = standardDeviation;\n",
    "'use strict';\n\nvar SQRT_2PI = Math.sqrt(2 * Math.PI);\n\nfunction cumulativeDistribution(z) {\n    var sum = z,\n        tmp = z;\n\n    // 15 iterations are enough for 4-digit precision\n    for (var i = 1; i < 15; i++) {\n        tmp *= z * z / (2 * i + 1);\n        sum += tmp;\n    }\n    return Math.round((0.5 + (sum / SQRT_2PI) * Math.exp(-z * z / 2)) * 1e4) / 1e4;\n}\n\n/**\n * A standard normal table, also called the unit normal table or Z table,\n * is a mathematical table for the values of Φ (phi), which are the values of\n * the cumulative distribution function of the normal distribution.\n * It is used to find the probability that a statistic is observed below,\n * above, or between values on the standard normal distribution, and by\n * extension, any normal distribution.\n *\n * The probabilities are calculated using the\n * [Cumulative distribution function](https://en.wikipedia.org/wiki/Normal_distribution#Cumulative_distribution_function).\n * The table used is the cumulative, and not cumulative from 0 to mean\n * (even though the latter has 5 digits precision, instead of 4).\n */\nvar standardNormalTable = [];\n\nfor (var z = 0; z <= 3.09; z += 0.01) {\n    standardNormalTable.push(cumulativeDistribution(z));\n}\n\nmodule.exports = standardNormalTable;\n",
    "'use strict';\n\n/**\n * The [sum](https://en.wikipedia.org/wiki/Summation) of an array\n * is the result of adding all numbers together, starting from zero.\n *\n * This runs on `O(n)`, linear time in respect to the array\n *\n * @param {Array<number>} x input\n * @return {number} sum of all input numbers\n * @example\n * console.log(sum([1, 2, 3])); // 6\n */\nfunction sum(x) {\n    var value = 0;\n    for (var i = 0; i < x.length; i++) {\n        value += x[i];\n    }\n    return value;\n}\n\nmodule.exports = sum;\n",
    "'use strict';\n\nvar mean = require(21);\n\n/**\n * The sum of deviations to the Nth power.\n * When n=2 it's the sum of squared deviations.\n * When n=3 it's the sum of cubed deviations.\n *\n * @param {Array<number>} x\n * @param {number} n power\n * @returns {number} sum of nth power deviations\n * @example\n * var input = [1, 2, 3];\n * // since the variance of a set is the mean squared\n * // deviations, we can calculate that with sumNthPowerDeviations:\n * var variance = sumNthPowerDeviations(input) / input.length;\n */\nfunction sumNthPowerDeviations(x, n) {\n    var meanValue = mean(x),\n        sum = 0;\n\n    for (var i = 0; i < x.length; i++) {\n        sum += Math.pow(x[i] - meanValue, n);\n    }\n\n    return sum;\n}\n\nmodule.exports = sumNthPowerDeviations;\n",
    "'use strict';\n\nvar standardDeviation = require(43);\nvar mean = require(21);\n\n/**\n * This is to compute [a one-sample t-test](https://en.wikipedia.org/wiki/Student%27s_t-test#One-sample_t-test), comparing the mean\n * of a sample to a known value, x.\n *\n * in this case, we're trying to determine whether the\n * population mean is equal to the value that we know, which is `x`\n * here. usually the results here are used to look up a\n * [p-value](http://en.wikipedia.org/wiki/P-value), which, for\n * a certain level of significance, will let you determine that the\n * null hypothesis can or cannot be rejected.\n *\n * @param {Array<number>} sample an array of numbers as input\n * @param {number} x expected vale of the population mean\n * @returns {number} value\n * @example\n * tTest([1, 2, 3, 4, 5, 6], 3.385); //= 0.16494154\n */\nfunction tTest(sample, x) {\n    // The mean of the sample\n    var sampleMean = mean(sample);\n\n    // The standard deviation of the sample\n    var sd = standardDeviation(sample);\n\n    // Square root the length of the sample\n    var rootN = Math.sqrt(sample.length);\n\n    // Compute the known value against the sample,\n    // returning the t value\n    return (sampleMean - x) / (sd / rootN);\n}\n\nmodule.exports = tTest;\n",
    "'use strict';\n\nvar mean = require(21);\nvar sampleVariance = require(39);\n\n/**\n * This is to compute [two sample t-test](http://en.wikipedia.org/wiki/Student's_t-test).\n * Tests whether \"mean(X)-mean(Y) = difference\", (\n * in the most common case, we often have `difference == 0` to test if two samples\n * are likely to be taken from populations with the same mean value) with\n * no prior knowledge on standard deviations of both samples\n * other than the fact that they have the same standard deviation.\n *\n * Usually the results here are used to look up a\n * [p-value](http://en.wikipedia.org/wiki/P-value), which, for\n * a certain level of significance, will let you determine that the\n * null hypothesis can or cannot be rejected.\n *\n * `diff` can be omitted if it equals 0.\n *\n * [This is used to confirm or deny](http://www.monarchlab.org/Lab/Research/Stats/2SampleT.aspx)\n * a null hypothesis that the two populations that have been sampled into\n * `sampleX` and `sampleY` are equal to each other.\n *\n * @param {Array<number>} sampleX a sample as an array of numbers\n * @param {Array<number>} sampleY a sample as an array of numbers\n * @param {number} [difference=0]\n * @returns {number} test result\n * @example\n * ss.tTestTwoSample([1, 2, 3, 4], [3, 4, 5, 6], 0); //= -2.1908902300206643\n */\nfunction tTestTwoSample(sampleX, sampleY, difference) {\n    var n = sampleX.length,\n        m = sampleY.length;\n\n    // If either sample doesn't actually have any values, we can't\n    // compute this at all, so we return `null`.\n    if (!n || !m) { return null; }\n\n    // default difference (mu) is zero\n    if (!difference) {\n        difference = 0;\n    }\n\n    var meanX = mean(sampleX),\n        meanY = mean(sampleY);\n\n    var weightedVariance = ((n - 1) * sampleVariance(sampleX) +\n        (m - 1) * sampleVariance(sampleY)) / (n + m - 2);\n\n    return (meanX - meanY - difference) /\n        Math.sqrt(weightedVariance * (1 / n + 1 / m));\n}\n\nmodule.exports = tTestTwoSample;\n",
    "'use strict';\n\nvar sumNthPowerDeviations = require(46);\n\n/**\n * The [variance](http://en.wikipedia.org/wiki/Variance)\n * is the sum of squared deviations from the mean.\n *\n * This is an implementation of variance, not sample variance:\n * see the `sampleVariance` method if you want a sample measure.\n *\n * @param {Array<number>} x a population\n * @returns {number} variance: a value greater than or equal to zero.\n * zero indicates that all values are identical.\n * @example\n * ss.variance([1, 2, 3, 4, 5, 6]); //= 2.917\n */\nfunction variance(x) {\n    // The variance of no numbers is null\n    if (x.length === 0) { return null; }\n\n    // Find the mean of squared deviations between the\n    // mean value and each value.\n    return sumNthPowerDeviations(x, 2) / x.length;\n}\n\nmodule.exports = variance;\n",
    "'use strict';\n\n/**\n * The [Z-Score, or Standard Score](http://en.wikipedia.org/wiki/Standard_score).\n *\n * The standard score is the number of standard deviations an observation\n * or datum is above or below the mean. Thus, a positive standard score\n * represents a datum above the mean, while a negative standard score\n * represents a datum below the mean. It is a dimensionless quantity\n * obtained by subtracting the population mean from an individual raw\n * score and then dividing the difference by the population standard\n * deviation.\n *\n * The z-score is only defined if one knows the population parameters;\n * if one only has a sample set, then the analogous computation with\n * sample mean and sample standard deviation yields the\n * Student's t-statistic.\n *\n * @param {number} x\n * @param {number} mean\n * @param {number} standardDeviation\n * @return {number} z score\n * @example\n * ss.zScore(78, 80, 5); //= -0.4\n */\nfunction zScore(x, mean, standardDeviation) {\n    return (x - mean) / standardDeviation;\n}\n\nmodule.exports = zScore;\n"
  ]
}